{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras import optimizers\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.layers import Dense, LSTM, SimpleRNN, RepeatVector, TimeDistributed, Flatten\n",
    "from keras import regularizers\n",
    "from keras.layers import Dropout\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Bidirectional, LSTM\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import chart_studio.plotly as py\n",
    "import plotly.graph_objects as go  # ou plotly.graph_objs, dependendo da versão\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "\n",
    "# Basic packages\n",
    "import datetime # manipulating date formats\n",
    "import seaborn as sns # for prettier plots\n",
    "\n",
    "\n",
    "# TIME SERIES\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "from statsmodels.tsa.stattools import adfuller, acf, pacf,arma_order_select_ic\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.tsa.api as smt\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as scs\n",
    "\n",
    "\n",
    "# settings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "# Set seeds to make the experiment more reproducible.\n",
    "from tensorflow.random import set_seed\n",
    "from numpy.random import seed\n",
    "set_seed(1)\n",
    "seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré-processamento dos dados\n",
    "\n",
    "* Carregando os dados\n",
    "* Padronizando os dados\n",
    "* Dividindo em treino, teste e validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('station_rio.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = data.filter([\"TEMP\"])\n",
    "temp_values = temp.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range = (0,1))\n",
    "scaled_data = scaler.fit_transform(temp_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data = temp_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo a proporção para treino, validação e teste\n",
    "train_size = 0.7\n",
    "valid_size = 0.15\n",
    "test_size = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total de dados\n",
    "total_data_len = len(scaled_data)\n",
    "train_data_len = int(np.floor(total_data_len * train_size))\n",
    "valid_data_len = int(np.floor(total_data_len * valid_size))\n",
    "print(total_data_len)\n",
    "print(train_data_len)\n",
    "print(valid_data_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando conjuntos de treinamento\n",
    "train_data = scaled_data[0:train_data_len, :]\n",
    "X_train, Y_train = [], []\n",
    "for i in range(60, len(train_data)):\n",
    "    X_train.append(train_data[i-60:i, 0])\n",
    "    Y_train.append(train_data[i, 0])\n",
    "\n",
    "X_train, Y_train = np.array(X_train), np.array(Y_train)\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "print(len(X_train))\n",
    "print(len(Y_train))\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o conjunto de validação\n",
    "valid_data = scaled_data[train_data_len:train_data_len + valid_data_len, :]\n",
    "X_valid, Y_valid = [], []\n",
    "\n",
    "for i in range(60, len(valid_data)):\n",
    "    X_valid.append(valid_data[i-60:i, 0])\n",
    "    Y_valid.append(valid_data[i, 0])\n",
    "\n",
    "X_valid, y_valid = np.array(X_valid), np.array(Y_valid)\n",
    "X_valid = np.reshape(X_valid, (X_valid.shape[0], X_valid.shape[1], 1))\n",
    "Y_valid = np.array(Y_valid)\n",
    "\n",
    "print(len(X_valid))\n",
    "print(len(Y_valid))\n",
    "print(len(valid_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustando o conjunto de teste para garantir que o tamanho de X_test e Y_test sejam iguais\n",
    "test_data = scaled_data[train_data_len + valid_data_len - 60:, :]  # Substitua a linha anterior\n",
    "Y_test = temp_values[train_data_len + valid_data_len:, :]  # Permanece o mesmo\n",
    "\n",
    "X_test = []\n",
    "for i in range(60, len(test_data)):\n",
    "    X_test.append(test_data[i-60:i, 0])\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "print(len(X_test))  # Agora deve corresponder ao tamanho de Y_test\n",
    "print(len(Y_test))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'X_train length: {len(X_train)}')\n",
    "print(f'Y_train length: {len(Y_train)}')\n",
    "print(f'X_valid length: {len(X_valid)}')\n",
    "print(f'y_valid length: {len(y_valid)}')\n",
    "print(f'X_test length: {len(X_test)}')\n",
    "print(f'Y_test length: {len(Y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decomposição da Série"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additive model\n",
    "res = sm.tsa.seasonal_decompose(df_rio_intp,period=12,model=\"additive\")\n",
    "#plt.figure(figsize=(16,12))\n",
    "fig = res.plot()\n",
    "#fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stationarity tests\n",
    "def test_stationarity(df_rio_intp):\n",
    "    \n",
    "    #Perform Dickey-Fuller test:\n",
    "    print('Results of Dickey-Fuller Test:')\n",
    "    dftest = adfuller(df_rio_intp, autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "    for key,value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)'%key] = value\n",
    "    print (dfoutput)\n",
    "\n",
    "test_stationarity(df_rio_intp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP for Time Series Forecasting\n",
    "\n",
    "* First we will use a Multilayer Perceptron model or MLP model, here our model will have input features equal to the window size.\n",
    "* The thing with MLP models is that the model don't take the input as sequenced data, so for the model, it is just receiving inputs and don't treat them as sequenced data, that may be a problem since the model won't see the data with the sequence patter that it has.\n",
    "* Input shape **[samples, timesteps]**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch = 32\n",
    "lr = 0.001\n",
    "adam = optimizers.Adam(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mlp = Sequential()\n",
    "model_mlp.add(Dense(64, activation='relu', input_dim=X_train.shape[1]))\n",
    "model_mlp.add(Dense(32))\n",
    "model_mlp.add(Dense(1))\n",
    "model_mlp.compile(loss='mean_squared_error', optimizer=adam)\n",
    "model_mlp.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_history = model_mlp.fit(X_train, Y_train, \n",
    "                            validation_data=(X_valid, Y_valid), \n",
    "                            epochs=epochs, \n",
    "                            verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_reg = Sequential()\n",
    "model_reg.add(Dense(64, activation='relu', input_dim=X_train.shape[1], \n",
    "                     kernel_regularizer=regularizers.l2(0.01)))  # Regularização L2\n",
    "model_reg.add(Dense(32, activation='relu', \n",
    "                     kernel_regularizer=regularizers.l2(0.01)))  # Regularização L2\n",
    "model_reg.add(Dense(1))  # Saída\n",
    "model_reg.compile(loss='mean_squared_error', optimizer=optimizers.Adam())\n",
    "model_reg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_reg_history = model_reg.fit(X_train, Y_train, \n",
    "                            validation_data=(X_valid, Y_valid), \n",
    "                            epochs=epochs, \n",
    "                            verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dropout = Sequential()\n",
    "model_dropout.add(Dense(64, activation='relu', input_dim=X_train.shape[1]))\n",
    "model_dropout.add(Dropout(0.5))  # 50% de dropout\n",
    "model_dropout.add(Dense(32, activation='relu'))\n",
    "model_dropout.add(Dropout(0.5))  # 50% de dropout\n",
    "model_dropout.add(Dense(1))  # Saída\n",
    "model_dropout.compile(loss='mean_squared_error', optimizer=optimizers.Adam())\n",
    "model_dropout.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_drop_history = model_dropout.fit(X_train, Y_train, \n",
    "                            validation_data=(X_valid, Y_valid), \n",
    "                            epochs=epochs, \n",
    "                            verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparando os 3 modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Função para calcular e exibir as métricas de cada modelo\n",
    "def evaluate_model(model, X_test, Y_test, model_name=\"Modelo\"):\n",
    "    # Realiza as previsões no conjunto de teste\n",
    "    Y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calcula as métricas\n",
    "    mae = mean_absolute_error(Y_test, Y_pred)\n",
    "    mse = mean_squared_error(Y_test, Y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    # Exibe os resultados\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"MSE: {mse:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    \n",
    "    return mae, mse, rmse\n",
    "\n",
    "# Avaliação de cada modelo no conjunto de teste\n",
    "mae_standard, mse_standard, rmse_standard = evaluate_model(model_mlp, X_test, Y_test, \"Modelo Padrão\")\n",
    "mae_reg, mse_reg, rmse_reg = evaluate_model(model_reg, X_test, Y_test, \"Modelo com Regularização\")\n",
    "mae_dropout, mse_dropout, rmse_dropout = evaluate_model(model_dropout, X_test, Y_test, \"Modelo com Dropout\")\n",
    "\n",
    "# Comparação dos resultados (opcional)\n",
    "print(\"\\nComparação de resultados:\")\n",
    "print(f\"Modelo Padrão - MAE: {mae_standard:.4f}, RMSE: {rmse_standard:.4f}\")\n",
    "print(f\"Modelo com Regularização - MAE: {mae_reg:.4f}, RMSE: {rmse_reg:.4f}\")\n",
    "print(f\"Modelo com Dropout - MAE: {mae_dropout:.4f}, RMSE: {rmse_dropout:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráficos de ajuste combinado no conjunto de treinamento e validação e previsão no conjunto de teste\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Modelo Simples\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(mlp_history.history['loss'], label='Treinamento')\n",
    "plt.plot(mlp_history.history['val_loss'], label='Validação')\n",
    "plt.title('Modelo Simples: Perda durante o Treinamento')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Perda')\n",
    "plt.legend()\n",
    "\n",
    "# Modelo com Regularização\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(mlp_reg_history.history['loss'], label='Treinamento')\n",
    "plt.plot(mlp_reg_history.history['val_loss'], label='Validação')\n",
    "plt.title('Modelo com Regularização: Perda durante o Treinamento')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Perda')\n",
    "plt.legend()\n",
    "\n",
    "# Modelo com Dropout\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(mlp_drop_history.history['loss'], label='Treinamento')\n",
    "plt.plot(mlp_drop_history.history['val_loss'], label='Validação')\n",
    "plt.title('Modelo com Dropout: Perda durante o Treinamento')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Perda')\n",
    "plt.legend()\n",
    "\n",
    "# # Modelo com Regularização e Dropout\n",
    "# plt.subplot(2, 2, 4)\n",
    "# plt.plot(cnn_regdrop_history.history['loss'], label='Treinamento')\n",
    "# plt.plot(cnn_regdrop_history.history['val_loss'], label='Validação')\n",
    "# plt.title('Modelo com Regularização e Dropout: Perda durante o Treinamento')\n",
    "# plt.xlabel('Épocas')\n",
    "# plt.ylabel('Perda')\n",
    "# plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine os dados de treino e validação em uma série para comparação visual com o conjunto de teste e previsões.\n",
    "train_valid_data = np.concatenate([Y_train, Y_valid])  # Combina Y_train e Y_valid em uma série contínua\n",
    "\n",
    "def plot_fit_with_predictions(train_valid_data, y_test, y_pred, model_name):\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    \n",
    "    # Plot dos dados de treino e validação\n",
    "    plt.plot(range(len(train_valid_data)), train_valid_data, label=\"Dados de Treinamento e Validação\", color=\"blue\")\n",
    "    \n",
    "    # Plot dos dados de teste\n",
    "    plt.plot(range(len(train_valid_data), len(train_valid_data) + len(y_test)), y_test, label=\"Dados de Teste\", color=\"green\")\n",
    "    \n",
    "    # Plot das previsões\n",
    "    plt.plot(range(len(train_valid_data), len(train_valid_data) + len(y_test)), y_pred, label=f\"Previsão {model_name}\", color=\"red\", linestyle=\"--\")\n",
    "    \n",
    "    plt.title(f\"Ajuste do Modelo com Previsões: {model_name}\")\n",
    "    plt.xlabel(\"Tempo\")\n",
    "    plt.ylabel(\"Valor\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Plot do ajuste e previsão para cada modelo\n",
    "plot_fit_with_predictions(train_valid_data, Y_test, y_pred_mlp, \"MLP Padrão\")\n",
    "plot_fit_with_predictions(train_valid_data, Y_test, y_pred_reg, \"MLP com Regularização\")\n",
    "plot_fit_with_predictions(train_valid_data, Y_test, y_pred_drop, \"MLP com Dropout\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN for Time Series Forecasting\n",
    "\n",
    "* For the CNN model we will use one convolutional hidden layer followed by a max pooling layer. The filter maps are then flattened before being interpreted by a Dense layer and outputting a prediction.\n",
    "* The convolutional layer should be able to identify patterns between the timesteps.\n",
    "* Input shape **[samples, timesteps, features]**.\n",
    "\n",
    "#### Data preprocess\n",
    "* Reshape from [samples, timesteps] into [samples, timesteps, features].\n",
    "* This same reshaped data will be used on the CNN and the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hiperparametros\n",
    "epochs = 20\n",
    "batch = 32\n",
    "lr = 0.001\n",
    "#adam = optimizers.Adam(lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_series = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_valid_series = X_valid.reshape((X_valid.shape[0], X_valid.shape[1], 1))\n",
    "X_test_series = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "print('Train set shape', X_train_series.shape)\n",
    "print('Validation set shape', X_valid_series.shape)\n",
    "print('Test set shape', X_test_series.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicando Modelo CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn = Sequential()\n",
    "model_cnn.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(X_train_series.shape[1], X_train_series.shape[2])))\n",
    "model_cnn.add(MaxPooling1D(pool_size=2))\n",
    "model_cnn.add(Flatten())\n",
    "model_cnn.add(Dense(50, activation='relu'))\n",
    "model_cnn.add(Dense(1))\n",
    "model_cnn.compile(loss='mse', optimizer=optimizers.Adam())\n",
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_history = model_cnn.fit(X_train_series, Y_train, \n",
    "                            validation_data=(X_valid_series, Y_valid), \n",
    "                            epochs=epochs, \n",
    "                            verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicando Regularização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn_reg = Sequential()\n",
    "model_cnn_reg.add(Conv1D(filters=64, kernel_size=2, activation='relu', \n",
    "                         kernel_regularizer=regularizers.l2(0.01),  # Regularização L2\n",
    "                         input_shape=(X_train_series.shape[1], X_train_series.shape[2])))\n",
    "model_cnn_reg.add(MaxPooling1D(pool_size=2))\n",
    "model_cnn_reg.add(Flatten())\n",
    "model_cnn_reg.add(Dense(50, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "model_cnn_reg.add(Dense(1))\n",
    "model_cnn_reg.compile(loss='mse', optimizer=optimizers.Adam())\n",
    "model_cnn_reg.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_reg_history = model_cnn_reg.fit(X_train_series, Y_train, \n",
    "                            validation_data=(X_valid_series, Y_valid), \n",
    "                            epochs=epochs, \n",
    "                            verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicando Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn_drop = Sequential()\n",
    "model_cnn_drop.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(X_train_series.shape[1], X_train_series.shape[2])))\n",
    "model_cnn_drop.add(MaxPooling1D(pool_size=2))\n",
    "model_cnn_drop.add(Dropout(0.5))  # Dropout após camada de pooling\n",
    "model_cnn_drop.add(Flatten())\n",
    "model_cnn_drop.add(Dense(50, activation='relu'))\n",
    "model_cnn_drop.add(Dropout(0.5))  # Dropout após camada densa\n",
    "model_cnn_drop.add(Dense(1))\n",
    "model_cnn_drop.compile(loss='mse', optimizer=optimizers.Adam())\n",
    "model_cnn_drop.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_drop_history = model_cnn_drop.fit(X_train_series, Y_train, \n",
    "                                       validation_data=(X_valid_series, Y_valid), \n",
    "                                       epochs=epochs, \n",
    "                                       verbose=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicando Regularização e Dropout combinados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn_regdrop = Sequential()\n",
    "model_cnn_regdrop.add(Conv1D(filters=64, kernel_size=2, activation='relu', \n",
    "                             kernel_regularizer=regularizers.l2(0.01),\n",
    "                             input_shape=(X_train_series.shape[1], X_train_series.shape[2])))\n",
    "model_cnn_regdrop.add(MaxPooling1D(pool_size=2))\n",
    "model_cnn_regdrop.add(Dropout(0.5))\n",
    "model_cnn_regdrop.add(Flatten())\n",
    "model_cnn_regdrop.add(Dense(50, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "model_cnn_regdrop.add(Dropout(0.5))\n",
    "model_cnn_regdrop.add(Dense(1))\n",
    "model_cnn_regdrop.compile(loss='mse', optimizer=optimizers.Adam())\n",
    "model_cnn_regdrop.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_regdrop_history = model_cnn_regdrop.fit(X_train_series, Y_train, \n",
    "                                             validation_data=(X_valid_series, Y_valid), \n",
    "                                             epochs=epochs, \n",
    "                                             verbose=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preivsao dos Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previsões\n",
    "predictions_simple = model_cnn.predict(X_test)\n",
    "predictions_reg = model_cnn_reg.predict(X_test)\n",
    "predictions_drop = model_cnn_drop.predict(X_test)\n",
    "predictions_regdrop = model_cnn_regdrop.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para calcular MAPE\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "# Função para calcular MSE\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "# Função para calcular RMSE\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "# Cálculo das métricas para cada modelo\n",
    "metrics = {\n",
    "    \"Modelo Simples\": {\n",
    "        \"MAPE\": mean_absolute_percentage_error(Y_test, predictions_simple),\n",
    "        \"MSE\": mean_squared_error(Y_test, predictions_simple),\n",
    "        \"RMSE\": root_mean_squared_error(Y_test, predictions_simple)\n",
    "    },\n",
    "    \"Modelo com Regularização\": {\n",
    "        \"MAPE\": mean_absolute_percentage_error(Y_test, predictions_reg),\n",
    "        \"MSE\": mean_squared_error(Y_test, predictions_reg),\n",
    "        \"RMSE\": root_mean_squared_error(Y_test, predictions_reg)\n",
    "    },\n",
    "    \"Modelo com Dropout\": {\n",
    "        \"MAPE\": mean_absolute_percentage_error(Y_test, predictions_drop),\n",
    "        \"MSE\": mean_squared_error(Y_test, predictions_drop),\n",
    "        \"RMSE\": root_mean_squared_error(Y_test, predictions_drop)\n",
    "    },\n",
    "    \"Modelo com Regularização e Dropout\": {\n",
    "        \"MAPE\": mean_absolute_percentage_error(Y_test, predictions_regdrop),\n",
    "        \"MSE\": mean_squared_error(Y_test, predictions_regdrop),\n",
    "        \"RMSE\": root_mean_squared_error(Y_test, predictions_regdrop)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Exibir as métricas\n",
    "for model, values in metrics.items():\n",
    "    print(f\"{model}: MAPE={values['MAPE']:.2f}%, MSE={values['MSE']:.4f}, RMSE={values['RMSE']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráficos de perda durante o treinamento\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Modelo Simples\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(cnn_history.history['loss'], label='Treinamento')\n",
    "plt.plot(cnn_history.history['val_loss'], label='Validação')\n",
    "plt.title('Modelo Simples: Perda durante o Treinamento')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Perda')\n",
    "plt.legend()\n",
    "\n",
    "# Modelo com Regularização\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(cnn_reg_history.history['loss'], label='Treinamento')\n",
    "plt.plot(cnn_reg_history.history['val_loss'], label='Validação')\n",
    "plt.title('Modelo com Regularização: Perda durante o Treinamento')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Perda')\n",
    "plt.legend()\n",
    "\n",
    "# Modelo com Dropout\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(cnn_drop_history.history['loss'], label='Treinamento')\n",
    "plt.plot(cnn_drop_history.history['val_loss'], label='Validação')\n",
    "plt.title('Modelo com Dropout: Perda durante o Treinamento')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Perda')\n",
    "plt.legend()\n",
    "\n",
    "# Modelo com Regularização e Dropout\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(cnn_regdrop_history.history['loss'], label='Treinamento')\n",
    "plt.plot(cnn_regdrop_history.history['val_loss'], label='Validação')\n",
    "plt.title('Modelo com Regularização e Dropout: Perda durante o Treinamento')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Perda')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráficos de predição\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Modelo Simples\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(Y_test, label='Real', color='blue')\n",
    "plt.plot(predictions_simple, label='Predito (Simples)', color='red')\n",
    "plt.title('Modelo Simples')\n",
    "plt.xlabel('Tempo')\n",
    "plt.ylabel('Valor')\n",
    "plt.legend()\n",
    "\n",
    "# Modelo com Regularização\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(Y_test, label='Real', color='blue')\n",
    "plt.plot(predictions_reg, label='Predito (Reg)', color='red')\n",
    "plt.title('Modelo com Regularização')\n",
    "plt.xlabel('Tempo')\n",
    "plt.ylabel('Valor')\n",
    "plt.legend()\n",
    "\n",
    "# Modelo com Dropout\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(Y_test, label='Real', color='blue')\n",
    "plt.plot(predictions_drop, label='Predito (Drop)', color='red')\n",
    "plt.title('Modelo com Dropout')\n",
    "plt.xlabel('Tempo')\n",
    "plt.ylabel('Valor')\n",
    "plt.legend()\n",
    "\n",
    "# Modelo com Regularização e Dropout\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(Y_test, label='Real', color='blue')\n",
    "plt.plot(predictions_regdrop, label='Predito (Reg + Drop)', color='red')\n",
    "plt.title('Modelo com Regularização e Dropout')\n",
    "plt.xlabel('Tempo')\n",
    "plt.ylabel('Valor')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráficos de ajuste no conjunto de treinamento e previsão no conjunto de teste\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Modelo Simples\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(Y_train, label='Real (Treino)', color='blue')  # Ajuste no conjunto de treino\n",
    "plt.plot(np.arange(len(Y_train), len(Y_train) + len(predictions_simple)), predictions_simple, label='Predito (Teste)', color='red')  # Previsão no conjunto de teste\n",
    "plt.title('Modelo Simples: Ajuste (Treino) e Previsão (Teste)')\n",
    "plt.xlabel('Tempo')\n",
    "plt.ylabel('Valor')\n",
    "plt.legend()\n",
    "\n",
    "# Modelo com Regularização\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(Y_train, label='Real (Treino)', color='blue')  # Ajuste no conjunto de treino\n",
    "plt.plot(np.arange(len(Y_train), len(Y_train) + len(predictions_reg)), predictions_reg, label='Predito (Teste)', color='red')  # Previsão no conjunto de teste\n",
    "plt.title('Modelo com Regularização: Ajuste (Treino) e Previsão (Teste)')\n",
    "plt.xlabel('Tempo')\n",
    "plt.ylabel('Valor')\n",
    "plt.legend()\n",
    "\n",
    "# Modelo com Dropout\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(Y_train, label='Real (Treino)', color='blue')  # Ajuste no conjunto de treino\n",
    "plt.plot(np.arange(len(Y_train), len(Y_train) + len(predictions_drop)), predictions_drop, label='Predito (Teste)', color='red')  # Previsão no conjunto de teste\n",
    "plt.title('Modelo com Dropout: Ajuste (Treino) e Previsão (Teste)')\n",
    "plt.xlabel('Tempo')\n",
    "plt.ylabel('Valor')\n",
    "plt.legend()\n",
    "\n",
    "# Modelo com Regularização e Dropout\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(Y_train, label='Real (Treino)', color='blue')  # Ajuste no conjunto de treino\n",
    "plt.plot(np.arange(len(Y_train), len(Y_train) + len(predictions_regdrop)), predictions_regdrop, label='Predito (Teste)', color='red')  # Previsão no conjunto de teste\n",
    "plt.title('Modelo com Regularização e Dropout: Ajuste (Treino) e Previsão (Teste)')\n",
    "plt.xlabel('Tempo')\n",
    "plt.ylabel('Valor')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine os dados de treino e validação em uma série para comparação visual com o conjunto de teste e previsões.\n",
    "train_valid_data = np.concatenate([Y_train, Y_valid])  # Combina Y_train e Y_valid em uma série contínua\n",
    "\n",
    "def plot_fit_with_predictions(train_valid_data, y_test, y_pred, model_name):\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    \n",
    "    # Plot dos dados de treino e validação\n",
    "    plt.plot(range(len(train_valid_data)), train_valid_data, label=\"Dados de Treinamento e Validação\", color=\"blue\")\n",
    "    \n",
    "    # Plot dos dados de teste\n",
    "    plt.plot(range(len(train_valid_data), len(train_valid_data) + len(y_test)), y_test, label=\"Dados de Teste\", color=\"green\")\n",
    "    \n",
    "    # Plot das previsões\n",
    "    plt.plot(range(len(train_valid_data), len(train_valid_data) + len(y_test)), y_pred, label=f\"Previsão {model_name}\", color=\"red\", linestyle=\"--\")\n",
    "    \n",
    "    plt.title(f\"Ajuste do Modelo com Previsões: {model_name}\")\n",
    "    plt.xlabel(\"Tempo\")\n",
    "    plt.ylabel(\"Valor\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Plot do ajuste e previsão para cada modelo\n",
    "plot_fit_with_predictions(train_valid_data, Y_test, predictions_simple, \"CNN Padrão\")\n",
    "plot_fit_with_predictions(train_valid_data, Y_test, predictions_reg, \"CNN com Regularização\")\n",
    "plot_fit_with_predictions(train_valid_data, Y_test, predictions_drop, \"CNN com Dropout\")\n",
    "plot_fit_with_predictions(train_valid_data, Y_test, predictions_regdrop, \"CNN com Regularização e Dropout\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM for Time Series Forecasting\n",
    "\n",
    "* Now the LSTM model actually sees the input data as a sequence, so it's able to learn patterns from sequenced data (assuming it exists) better than the other ones, especially patterns from long sequences.\n",
    "* Input shape **[samples, timesteps, features]**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_9\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_9\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,400</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │        \u001b[38;5;34m10,400\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_19 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m51\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,451</span> (40.82 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m10,451\u001b[0m (40.82 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,451</span> (40.82 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m10,451\u001b[0m (40.82 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_lstm = Sequential()\n",
    "model_lstm.add(LSTM(50, activation='tanh', input_shape=(X_train_series.shape[1], X_train_series.shape[2])))\n",
    "model_lstm.add(Dense(1))\n",
    "model_lstm.compile(loss='mse', optimizer=optimizers.Adam())\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "11/11 - 3s - 276ms/step - loss: 612.1807 - val_loss: 579.4474\n",
      "Epoch 2/15\n",
      "11/11 - 0s - 30ms/step - loss: 565.6359 - val_loss: 538.9734\n",
      "Epoch 3/15\n",
      "11/11 - 0s - 40ms/step - loss: 525.3078 - val_loss: 497.5480\n",
      "Epoch 4/15\n",
      "11/11 - 1s - 46ms/step - loss: 482.0794 - val_loss: 451.4311\n",
      "Epoch 5/15\n",
      "11/11 - 0s - 31ms/step - loss: 433.5675 - val_loss: 401.7878\n",
      "Epoch 6/15\n",
      "11/11 - 0s - 31ms/step - loss: 386.3129 - val_loss: 358.1352\n",
      "Epoch 7/15\n",
      "11/11 - 0s - 21ms/step - loss: 344.1666 - val_loss: 319.0901\n",
      "Epoch 8/15\n",
      "11/11 - 0s - 28ms/step - loss: 307.2392 - val_loss: 285.3972\n",
      "Epoch 9/15\n",
      "11/11 - 0s - 24ms/step - loss: 273.3243 - val_loss: 249.5125\n",
      "Epoch 10/15\n",
      "11/11 - 0s - 23ms/step - loss: 235.9312 - val_loss: 213.0459\n",
      "Epoch 11/15\n",
      "11/11 - 0s - 29ms/step - loss: 200.9870 - val_loss: 180.3166\n",
      "Epoch 12/15\n",
      "11/11 - 0s - 33ms/step - loss: 170.4737 - val_loss: 154.6104\n",
      "Epoch 13/15\n",
      "11/11 - 0s - 24ms/step - loss: 148.7137 - val_loss: 136.6457\n",
      "Epoch 14/15\n",
      "11/11 - 0s - 24ms/step - loss: 130.3198 - val_loss: 115.9113\n",
      "Epoch 15/15\n",
      "11/11 - 0s - 28ms/step - loss: 109.8015 - val_loss: 99.3184\n"
     ]
    }
   ],
   "source": [
    "lstm_history = model_lstm.fit(X_train_series, Y_train, validation_data=(X_valid_series, Y_valid), epochs=15, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN-LSTM for Time Series Forecasting\n",
    "* Input shape **[samples, subsequences, timesteps, features]**.\n",
    "\n",
    "#### Model explanation from the [article](https://machinelearningmastery.com/how-to-get-started-with-deep-learning-for-time-series-forecasting-7-day-mini-course/)\n",
    "> \"The benefit of this model is that the model can support very long input sequences that can be read as blocks or subsequences by the CNN model, then pieced together by the LSTM model.\"\n",
    ">\n",
    "> \"When using a hybrid CNN-LSTM model, we will further divide each sample into further subsequences. The CNN model will interpret each sub-sequence and the LSTM will piece together the interpretations from the subsequences. As such, we will split each sample into 2 subsequences of 2 times per subsequence.\"\n",
    ">\n",
    "> \"The CNN will be defined to expect 2 timesteps per subsequence with one feature. The entire CNN model is then wrapped in TimeDistributed wrapper layers so that it can be applied to each subsequence in the sample. The results are then interpreted by the LSTM layer before the model outputs a prediction.\"\n",
    "\n",
    "#### Data preprocess\n",
    "* Reshape from [samples, timesteps, features] into [samples, subsequences, timesteps, features]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape (334, 2, 30, 1)\n",
      "Validation set shape (24, 2, 30, 1)\n"
     ]
    }
   ],
   "source": [
    "subsequences = 2\n",
    "timesteps = X_train_series.shape[1]//subsequences\n",
    "X_train_series_sub = X_train_series.reshape((X_train_series.shape[0], subsequences, timesteps, 1))\n",
    "X_valid_series_sub = X_valid_series.reshape((X_valid_series.shape[0], subsequences, timesteps, 1))\n",
    "print('Train set shape', X_train_series_sub.shape)\n",
    "print('Validation set shape', X_valid_series_sub.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn_lstm = Sequential()\n",
    "model_cnn_lstm.add(TimeDistributed(Conv1D(filters=64, kernel_size=1, activation='relu'), input_shape=(None, X_train_series_sub.shape[2], X_train_series_sub.shape[3])))\n",
    "model_cnn_lstm.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "model_cnn_lstm.add(TimeDistributed(Flatten()))\n",
    "model_cnn_lstm.add(LSTM(50, activation='relu'))\n",
    "model_cnn_lstm.add(Dense(1))\n",
    "model_cnn_lstm.compile(loss='mse', optimizer=optimizers.Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "11/11 - 4s - 359ms/step - loss: 98.5148 - val_loss: 4.9884\n",
      "Epoch 2/20\n",
      "11/11 - 0s - 13ms/step - loss: 6.5289 - val_loss: 6.2736\n",
      "Epoch 3/20\n",
      "11/11 - 0s - 15ms/step - loss: 6.4010 - val_loss: 5.0431\n",
      "Epoch 4/20\n",
      "11/11 - 0s - 14ms/step - loss: 5.4654 - val_loss: 5.2121\n",
      "Epoch 5/20\n",
      "11/11 - 0s - 13ms/step - loss: 4.6125 - val_loss: 4.3710\n",
      "Epoch 6/20\n",
      "11/11 - 0s - 15ms/step - loss: 4.2339 - val_loss: 3.8592\n",
      "Epoch 7/20\n",
      "11/11 - 0s - 13ms/step - loss: 3.9177 - val_loss: 3.4922\n",
      "Epoch 8/20\n",
      "11/11 - 0s - 16ms/step - loss: 3.5111 - val_loss: 3.3184\n",
      "Epoch 9/20\n",
      "11/11 - 0s - 14ms/step - loss: 2.9520 - val_loss: 2.5441\n",
      "Epoch 10/20\n",
      "11/11 - 0s - 12ms/step - loss: 2.2775 - val_loss: 1.6939\n",
      "Epoch 11/20\n",
      "11/11 - 0s - 14ms/step - loss: 1.8716 - val_loss: 1.3770\n",
      "Epoch 12/20\n",
      "11/11 - 0s - 14ms/step - loss: 1.7615 - val_loss: 1.2637\n",
      "Epoch 13/20\n",
      "11/11 - 0s - 13ms/step - loss: 1.7299 - val_loss: 1.2524\n",
      "Epoch 14/20\n",
      "11/11 - 0s - 14ms/step - loss: 1.7083 - val_loss: 1.2522\n",
      "Epoch 15/20\n",
      "11/11 - 0s - 15ms/step - loss: 1.6936 - val_loss: 1.2580\n",
      "Epoch 16/20\n",
      "11/11 - 0s - 15ms/step - loss: 1.6795 - val_loss: 1.2620\n",
      "Epoch 17/20\n",
      "11/11 - 0s - 15ms/step - loss: 1.6696 - val_loss: 1.2567\n",
      "Epoch 18/20\n",
      "11/11 - 0s - 13ms/step - loss: 1.6627 - val_loss: 1.2462\n",
      "Epoch 19/20\n",
      "11/11 - 0s - 13ms/step - loss: 1.6527 - val_loss: 1.2368\n",
      "Epoch 20/20\n",
      "11/11 - 0s - 12ms/step - loss: 1.6464 - val_loss: 1.2287\n"
     ]
    }
   ],
   "source": [
    "cnn_lstm_history = model_cnn_lstm.fit(X_train_series_sub, Y_train, validation_data=(X_valid_series_sub, Y_valid), epochs=epochs, verbose=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
